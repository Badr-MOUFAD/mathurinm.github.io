<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Syllabus of the &ldquo;Large scale optimization for machine learning&rdquo; class</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Syllabus of the &ldquo;Large scale optimization for machine learning&rdquo; class</h1>
</div>
<p>The goal of the class is to cover theoretical aspects and practical Python implementations of popular optimization algorithms in machine learning.
</p>
<p>At the end of the class, the students will:
</p>
<ul>
<li><p>be familiar with essential convex analysis notions
</p>
</li>
<li><p>be able to derive standard convergence proofs and convergence rates
</p>
</li>
<li><p>know which algorithm to chose given the problem to solve
</p>
</li>
<li><p>understand the implementation of these algorithms in Python
</p>
</li>
</ul>
<h2>Syllabus</h2>
<ul>
<li><p>basics of convex analysis: convex sets, convex functions, strong convexity, smoothness, subdifferential
</p>
</li>
<li><p>Fenchel-Legendre transform, infimal convolution, Moreau envelope
</p>
</li>
<li><p>gradient descent and subgradient descent (Lab 1)
</p>
</li>
<li><p>fixed point iterations, proximal point method
</p>
</li>
<li><p>acceleration of first order methods
</p>
</li>
<li><p>composite optimization: proximal gradient, applications to low complexity regularization (Lab 2)
</p>
</li>
<li><p>Moreau-Rockafellar duality, duality gap
</p>
</li>
<li><p>stochastic algorithms for empirical risk minimization: stochastic gradient descent, variance reduction methods (Lab 3)
</p>
</li>
<li><p>eventually: Frank-Wolfe algorithm, coordinate descent, primal-dual algorithms
</p>
</li>
</ul>
<h2>Schedule</h2>
<p>15 x 2 h of class/labs, 2 h written examination
</p>
<h2>Validation</h2>
<p>3 Labs and one written exam
</p>
<h2>Ressources</h2>
<ul>
<li><p>Introductory lectures on convex optimization: a basic course, Y. Nesterov, 2004. A reference book in optimization.
</p>
</li>
<li><p>First order methods in optimization, A. Beck, 2019. Maybe a bit more modern than Nesterov.
</p>
</li>
<li><p>Convex optimization: algorithms and complexity, S. Bubeck, 2015. A short monograph (100 pages) covering basic topics.
</p>
</li>
</ul>
<h2>Prerequisite</h2>
<ul>
<li><p>Differential calculus: gradient, Hessian
</p>
</li>
<li><p>Notions of convexity
</p>
</li>
<li><p>Linear algebra: eigenvalue decomposition, singular value decomposition</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2022-02-22 12:49:45 CET, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</div>
</body>
</html>
