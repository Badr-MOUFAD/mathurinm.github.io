<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Syllabus of the &ldquo;Large scale optimization for machine and deep learning&rdquo; class</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Syllabus of the &ldquo;Large scale optimization for machine and deep learning&rdquo; class</h1>
</div>
<p>The class studies the success of algorithms for large scale deep learning problems, covering both recent theoretical results as well as practical Python implementations of popular optimization algorithms.
</p>
<h2>Syllabus</h2>
<ul>
<li><p>basics of convex analysis: convex sets, convex functions, strong convexity, smoothness, subdifferential, Fenchel-Legendre transform, infimal convolution, Moreau envelope
</p>
</li>
<li><p>gradient descent and subgradient descent, fixed point iterations, proximal point method (Lab 1)
</p>
</li>
<li><p>acceleration of first order methods: Nesterov and momentum
</p>
</li>
<li><p>algorithms for Deep Learning: stochastic gradient descent, ADAM, Adagrad (Lab 2)
</p>
</li>
<li><p>automatic differentiation
</p>
</li>
<li><p>second order algorithms: Newton and quasi-Newton methods
</p>
</li>
<li><p>implicit regularization, duality, Bregman geometry, mirror descent
</p>
</li>
<li><p>recent results in non convex optimization
</p>
</li>
<li><p>other algorithms: Frank-Wolfe, primal-dual algorithms, extragradient.
</p>
</li>
</ul>
<h2>Schedule</h2>
<p>15 x 2 h of class/labs, oral presentation
</p>
<h2>Validation</h2>
<p>2 Labs, homeworks and one oral written exam
</p>
<h2>Ressources</h2>
<ul>
<li><p>Lectures on Convex Optimization: Y. Nesterov, 2018. An updated version of one of the reference books in optimization.
</p>
</li>
<li><p>First order methods in optimization, A. Beck, 2019.
</p>
</li>
<li><p>Convex optimization: algorithms and complexity, S. Bubeck, 2015. A short monograph (100 pages) covering basic topics.
</p>
</li>
</ul>
<h2>Prerequisite</h2>
<ul>
<li><p>Differential calculus: gradient, Hessian
</p>
</li>
<li><p>Notions of convexity
</p>
</li>
<li><p>Linear algebra: eigenvalue decomposition, singular value decomposition</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2023-05-22 11:34:42 CEST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</div>
</body>
</html>
