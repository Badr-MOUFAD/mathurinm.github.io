# jemdoc: menu{MENU}{teaching.html}, nofooter
== Teaching


Material for the two labs can be downloaded [https://filesender.renater.fr/?s=download&token=1d569af5-6651-43d3-9e5f-c35db82b0e5b from here].
Slides are [./assets/2023_olissipo/olissipo_slides.pdf here].


== Optimization for large scale Machine Learning, M2 ENS 2022-2023 & 2023-2024

The goal of the class is to cover theoretical aspects and practical Python implementations of popular optimization algorithms in machine learning, with a focus on modern topics: huge scale models, automatic differentiation, deep learning, implicit bias, etc.


*Notes for the class* in their 2022 version are [./assets/2022_ens/class.pdf here] and exercises are [./assets/2022_ens/exos.pdf here]

*Schedule*: From November 21st onwards: Tuesday 08 h 00, Friday 13 h 30 (room B1).

*Validation*: weekly theoretical homeworks, labs and paper presentation at the end of the class.

*Ressources*:
- /Introductory lectures on convex optimization: a basic course/, Y. Nesterov, 2004. A reference book in optimization, updated in 2018: /Lectures on Convex Optimization/.
- /First order methods in optimization/, A. Beck, 2019.
- /Convex optimization: algorithms and complexity/, S. Bubeck, 2015. A short monograph (100 pages) covering basic topics.

== OLISSIPO Winter school: dimensionality reduction (02/2023)


== Convex optimization @Computation and Modelling summer school, WUST 2022

- [./assets/2022_wust/slides_intro.pdf intro slides ]
- [./assets/2022_wust/exos.pdf exercises] (updated every day)
- [./assets/2022_wust/Lecture1_gd_ols.ipynb notebook 1 (Introduction to gradient descent on least squares)]
- [./assets/2022_wust/Lecture2_generic_GD.ipynb notebook 2 (Gradient descent on generic functions)]
- [./assets/2022_wust/Lecture3_nonsmooth.ipynb notebook 3 (Non smooth optimization: the proximal gradient algorithm)]
- [./assets/2022/wust/Lecture4_Newton_CD.ipynb notebook 4 (Alternatives to gradient descent)]

== Resources

My colleague [https://pierreablin.com/ Pierre Ablin] and I have created a repository with some Python advice for our students:
[https://github.com/pierreablin/python-sessions].


== Classes taught
Since 2019, I teach the [https://moodle.polytechnique.fr/course/view.php?id=7524 Python for datascience] class (42 h per year) in the X/HEC "Datascience for business" Master, using live coding  inspired by the Software Carpentry workshops. I designed the course from scratch, collaborating  with Joan Massich in 2019, Quentin Bertrand in 2020, and Hicham Janati in 2021.

Since 2020 I teach and handle practical sessions and data camps in Ecole Polytechnique's [https://portail.polytechnique.edu/datascience/en/programs/data-science-starter-program-dssp Executive education] (70 h).
Topics involved dimension reduction, clustering, scaling computations, visualization and datacamp. I designed 2 full python labs with Erwan Le Pennec on these topics.

From 2017 to 2019, my main teaching activity was the Optimization for datascience class of the [https://www.universite-paris-saclay.fr/formation/master/mathematiques-et-applications/m2-data-sciences  Datascience Master], totalling 2*40 h including 4 h as lecturer.
Amongst others, this involved refactoring of the practical sessions, tutoring of students during office hours, and partaking in the design of the final exam.

In 2016-2017, I was a TA at Télécom Paris, for
- Analysis and Probabilities (MDI 113/114, Bachelor, 10 h)
- Machine Learning and Data Mining (MDI 343, Executive Master, 20 h)
- Linear Models (SD 204, Master, 10 h)
- Practical Machine Learning (SD 207, Master, 10 h)
- Tools and applications for signals and images (SI 101, Bachelor, 6 h)
