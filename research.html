<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Research</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="research.html" class="current">Research</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-category">Useful links</div>
<div class="menu-item"><a href="https://github.com/mathurinm/">GitHub</a></div>
<div class="menu-item"><a href="https://stackoverflow.com/users/2902280/p-camilleri">StackOverflow</a></div>
<div class="menu-item"><a href="https://uk.linkedin.com/in/mathurin-massias-67434883">Linkedin</a></div>
<div class="menu-item"><a href="https://scholar.google.com/citations?user=kaTDZS0AAAAJ">Google&nbsp;Scholar</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Research</h1>
</div>
<h2>Papers</h2>
<ul>
<li><p>2024
</p>
<ul>
<li><p>A. Gagneux, <b>M. Massias</b>, E. Soubies, Automated and Unbiased Coefficient Clustering with Non Convex SLOPE, under review for TMLR.
</p>
</li>
<li><p>C. Pouliquen, <b>M. Massias</b>, T. Vayer, <a href="https://arxiv.org/abs/2406.09023" target=&ldquo;blank&rdquo;>Schur's Positive-Definite Network: Deep Learning in the SPD cone with structure</a>, submitted.
</p>
</li></ul>
</li>
<li><p>2023
</p>
<ul>
<li><p>J. Larsson, Q. Klopfenstein, <b>M. Massias</b>, J. Wallin, <a href="https://arxiv.org/abs/2210.14780" target=&ldquo;blank&rdquo;>Coordinate descent for SLOPE</a>, AISTATS 2023.
</p>
</li>
<li><p>B. Moufad, P.-A. Bannier, Q. Bertrand, Q. Klopfenstein, <b>M. Massias</b>, <a href="./assets/pdf/skglm_mloss.pdf" target=&ldquo;blank&rdquo;>skglm: improving scikit-learn for regularized Generalized Linear Models</a>, submitted to JMLR OSS.
</p>
</li>
<li><p>C. Molinari, <b>M. Massias</b>, L. Rosasco, S. Villa, <a href="https://arxiv.org/abs/2202.00420" target=&ldquo;blank&rdquo;>Iterative regularization for low-complexity regularizers</a>, Numerische Mathematike.
</p>
</li>
<li><p>C. Pouliquen, P. Gonçalves, <b>M. Massias</b>, T. Vayer, <a href="Implicit" target=&ldquo;blank&rdquo;>Differentiation for Hyperparameter Tuning the Weighted Graphical Lasso https:<i></i>arxiv.org<i>abs</i>2307.02130</a>, GRETSI 2023.
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>2022
</p>
<ul>
<li><p>T. Moreau, <b>M. Massias</b>, A. Gramfort and others, <a href="https://arxiv.org/abs/2206.13424" target=&ldquo;blank&rdquo;>Benchopt: Reproducible, efficient and collaborative optimization benchmarks</a>, NeuRIPS 2022.
</p>
</li>
<li><p>Q. Bertrand, Q. Klopfenstein, P.-A. Bannier, G. Gidel, <b>M. Massias</b>, <a href="https://arxiv.org/abs/2204.07826" target=&ldquo;blank&rdquo;>Beyond L1: Faster and better sparse models with skglm</a>, NeuRIPS 2022.
</p>
</li>
<li><p>B. Muzellec, K. Sato, <b>M. Massias</b>, T. Suzuki, <a href="https://arxiv.org/abs/2003.00306" target=&ldquo;blank&rdquo;>Dimension-free convergence rates for gradient Langevin dynamics in RKHS</a>, COLT 2022.
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>2021
</p>
<ul>
<li><p>Q. Bertrand, Q. Klopfenstein, <b>M. Massias</b>, M. Blondel, S. Vaiter, A. Gramfort, J. Salmon,
<a href="https://arxiv.org/abs/2105.01637" target=&ldquo;blank&rdquo;>Implicit differentiation for fast hyperparameter selection in non-smooth convex learning</a>, JMLR.
<a href="https://github.com/QB3/sparse-ho" target=&ldquo;blank&rdquo;>code</a>
</p>
</li>
<li><p>Q. Bertrand, <b>M. Massias</b>, <a href="https://arxiv.org/abs/2011.10065" target=&ldquo;blank&rdquo;>Anderson acceleration of coordinate descent</a>, AISTATS 2021. <a href="https://mathurinm.github.io/andersoncd" target=&ldquo;blank&rdquo;>code</a>
</p>
</li>
<li><p>C. Molinari, <b>M. Massias</b>, L. Rosasco, S. Villa,
<a href="https://arxiv.org/abs/2006.09859" target=&ldquo;blank&rdquo;>Iterative regularization for convex regularizers</a>, AISTATS 2021. <a href="https://LCSL.github.io/iterreg" target=&ldquo;blank&rdquo;>code</a>
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>2020
</p>
<ul>
<li><p><b>M. Massias</b>*, Q. Bertrand*, A. Gramfort, J. Salmon,
<a href="https://arxiv.org/abs/2001.05401" target=&ldquo;blank&rdquo;>Support recovery and sup-norm convergence rates for sparse pivotal estimation</a>, AISTATS 2020.
</p>
</li>
<li><p><b>M. Massias</b>, S. Vaiter, A. Gramfort, J. Salmon,
<a href="https://jmlr.org/papers/v21/19-587.html" target=&ldquo;blank&rdquo;>Dual extrapolation for sparse Generalized Linear Models</a>, JMLR. <a href="https://github.com/mathurinm/celer" target=&ldquo;blank&rdquo;>celer library</a>
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>2019
</p>
<ul>
<li><p>P. Ablin, T. Moreau, <b>M. Massias</b>, A. Gramfort,
<a href="https://arxiv.org/abs/1905.11071" target=&ldquo;blank&rdquo;>Learning step sizes for unfolded sparse coding</a>, NeurIPS 2019.
</p>
</li>
<li><p>Q. Bertrand*, <b>M. Massias</b>*, A. Gramfort, J. Salmon,
<a href="https://arxiv.org/abs/1902.02509" target=&ldquo;blank&rdquo;>Handling correlated and repeated measurements with the smoothed multivariate square-root Lasso</a>, NeurIPS 2019.
<a href="https://github.com/QBE/clar" target=&ldquo;blank&rdquo;>code</a>
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>2018
</p>
<ul>
<li><p><b>M. Massias</b>, A. Gramfort, J. Salmon,
<a href="http://proceedings.mlr.press/v80/massias18a.html" target=&ldquo;blank&rdquo;>Celer: a fast solver for the Lasso with dual extrapolation</a>,
ICML 2018.
<a href="https://drive.google.com/file/d/1doIoZ2dahVNhxFhLqHOwK_VfdOQrZjj6/view?usp=sharing" target=&ldquo;blank&rdquo;>slides</a>
<a href="https://github.com/mathurinm/celer" target=&ldquo;blank&rdquo;>code</a>
<a href="https://mathurinm.github.io/celer/" target=&ldquo;blank&rdquo;>doc</a>
</p>
</li>
<li><p><b>M. Massias</b>, O. Fercoq, A. Gramfort, J. Salmon,
<a href="http://proceedings.mlr.press/v84/massias18a.html" target=&ldquo;blank&rdquo;>Generalized
concomitant multi-task Lasso for sparse multimodal regression</a>,
AISTATS 2018.
<a href="https://github.com/mathurinm/SHCL" target=&ldquo;blank&rdquo;>code</a>
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>2017
</p>
<ul>
<li><p><b>M. Massias</b>, A. Gramfort, J. Salmon,
<a href="https://arxiv.org/abs/1703.07285" target=&ldquo;blank&rdquo;>From safe screening rules to working sets
for faster Lasso-type solvers</a>,
OPTML workshop at NIPS 2017.
<a href="https://github.com/mathurinm/A5G" target=&ldquo;blank&rdquo;>deprecated code</a>
</p>
</li>
<li><p><b>M. Massias</b>, J. Salmon, A. Gramfort,
<a href="http://spars2017.lx.it.pt/index_files/papers/SPARS2017_Paper_77.pdf" target=&ldquo;blank&rdquo;>Gap safe screening rules for faster complex-valued multi-task group Lasso</a>,
SPARS, Lisbon, 2017.
</p>
</li>
</ul>

</li>
</ul>
<h2>PhD. Thesis</h2>
<ul>
<li><p><b>M. Massias</b>, Sparse high dimension linear regression in the presence of heteroscedastic noise: application to magnetoelectric source imaging. Defended on 04<i>12</i>2019.
<a href="https://tel.archives-ouvertes.fr/tel-02401628" target=&ldquo;blank&rdquo;>manuscript</a>
<a href="./assets/pdf/slides_defense.pdf" target=&ldquo;blank&rdquo;>slides</a>
</p>
</li>
</ul>
<h2>Relevant slides</h2>
<ul>
<li><p><a href="./assets/pdf/cirm_mathurin.pdf" target=&ldquo;blank&rdquo;>Dual extrapolation</a>, 09032020, Optimization for Machine Learning workshop at CIRM, Luminy.
</p>
</li>
<li><p><a href="./assets/pdf/bounds_aistats20_slides.pdf" target=&ldquo;blank&rdquo;>LCSL group meeting</a>, 23012020, Università di Genova.
</p>
</li>
<li><p><a href="./assets/pdf/slides_defense.pdf" target=&ldquo;blank&rdquo;>PhD Defense</a>, 04122019, Inria.
</p>
</li>
<li><p><a href="./assets/pdf/AIP2019.pdf" target=&ldquo;blank&rdquo;>Dual extrapolation for sparse Generalized Linear Models</a>,
09072019, Applied Inverse Problems conference, Grenoble.
</p>
</li>
<li><p><a href="./assets/pdf/uw_mm.pdf" target=&ldquo;blank&rdquo;>Celer: fast solver for the Lasso with dual extrapolation</a>,
11052018, Statistics Seminar at University of Washington.
</p>
</li>
<li><p><a href="https://goo.gl/hZRwqi" target=&ldquo;blank&rdquo;>Generalized concomitant multi-task Lasso for sparse multimodal regression</a>,
20102017, Journées GDR MOA MIA (Bordeaux).
Also presented at PGMO days 2017, Saclay.
</p>
</li>
<li><p><a href="https://goo.gl/wACD2h" target=&ldquo;blank&rdquo;>Résolution rapide de problèmes de type Lasso: des règles de safe screening aux working sets</a> (in French), 05092017, GRETSI.
Also presented at JDSE 2017 (best presentation award).
</p>
</li>
<li><p><a href="https://goo.gl/8m0a8s" target=&ldquo;blank&rdquo;>Faster solvers for sparse multi-task problems</a>, 20032017, IAP (Paris).
Also presented at CMStats 2017, London.
</p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
